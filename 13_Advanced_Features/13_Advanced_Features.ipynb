{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† Advanced Features in FastAPI\n",
    "\n",
    "FastAPI supports **advanced patterns** ‚Äî like async I/O, background tasks, LLMs, and real-time updates ‚Äî to build powerful modern APIs.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Dependency Caching\n",
    "\n",
    "Use `Depends(..., use_cache=True)` to avoid recomputing **shared or expensive dependencies** within a request.\n",
    "\n",
    "```python\n",
    "from fastapi import Depends\n",
    "\n",
    "def get_settings():\n",
    "    print(\"Loading settings...\")\n",
    "    return {\"db\": \"postgres\"}\n",
    "\n",
    "@app.get(\"/info\")\n",
    "def read_info(settings=Depends(get_settings)):\n",
    "    return {\"db\": settings[\"db\"]}\n",
    "```\n",
    "\n",
    "‚è±Ô∏è Runs only **once per request**, even if used in multiple places.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Async + Sync Support\n",
    "\n",
    "FastAPI supports both sync (`def`) and async (`async def`) route functions.\n",
    "\n",
    "```python\n",
    "@app.get(\"/sync\")\n",
    "def sync_func():\n",
    "    return {\"type\": \"sync\"}\n",
    "\n",
    "@app.get(\"/async\")\n",
    "async def async_func():\n",
    "    return {\"type\": \"async\"}\n",
    "```\n",
    "\n",
    "‚úÖ Use `async def` when dealing with:\n",
    "\n",
    "* Async DB clients (e.g. SQLAlchemy async)\n",
    "* I/O tasks: file access, network calls, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Background Jobs with Celery\n",
    "\n",
    "Run **heavy or long-running tasks** (e.g. email, ML inference) outside the request cycle.\n",
    "\n",
    "```python\n",
    "from celery import Celery\n",
    "\n",
    "celery_app = Celery(\"worker\", broker=\"redis://localhost:6379/0\")\n",
    "\n",
    "@celery_app.task\n",
    "def send_email_task(email: str):\n",
    "    print(f\"Sending email to {email}\")\n",
    "```\n",
    "\n",
    "Trigger from FastAPI:\n",
    "\n",
    "```python\n",
    "@app.post(\"/send\")\n",
    "def trigger_task(email: str):\n",
    "    send_email_task.delay(email)\n",
    "    return {\"status\": \"Email scheduled\"}\n",
    "```\n",
    "\n",
    "‚öôÔ∏è Requires a separate **Celery worker** and **Redis broker** running.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ LangChain or OpenAI Integration\n",
    "\n",
    "Use LLM APIs directly inside FastAPI routes.\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from fastapi import Body\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\")\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def ask_llm(prompt: str = Body(...)):\n",
    "    response = chat.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "```\n",
    "\n",
    "üì¶ Works with other LLM providers too (Claude, Mistral, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ WebSocket + LLM Agents\n",
    "\n",
    "Enable **real-time chat** with language agents.\n",
    "\n",
    "```python\n",
    "from fastapi import WebSocket\n",
    "\n",
    "@app.websocket(\"/ws/ai-chat\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "    while True:\n",
    "        msg = await websocket.receive_text()\n",
    "        response = chat.invoke(msg)\n",
    "        await websocket.send_text(response.content)\n",
    "```\n",
    "\n",
    "üí¨ Ideal for live assistants or AI chat UIs.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Feature              | Purpose / Use Case                   |\n",
    "| -------------------- | ------------------------------------ |\n",
    "| Dependency Caching   | Prevent repeated heavy calculations  |\n",
    "| Async + Sync Support | Optimize CPU and I/O together        |\n",
    "| Celery Tasks         | Offload long-running background jobs |\n",
    "| LLM API Integration  | Add smart AI endpoints               |\n",
    "| WebSocket with LLMs  | Live interaction with AI agents      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
