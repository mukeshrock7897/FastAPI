{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìò Integration with LangChain\n",
    "\n",
    "Use **FastAPI** to serve **LangChain-powered LLM chains** as REST APIs.\n",
    "\n",
    "This setup is useful when you want:\n",
    "\n",
    "* ‚úÖ LangChain for LLM logic and chaining\n",
    "* ‚úÖ FastAPI for serving it via endpoints\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 1: Install Requirements\n",
    "\n",
    "Install LangChain, FastAPI, and Uvicorn:\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn langchain openai\n",
    "```\n",
    "\n",
    "> Also set your OpenAI API key (or other LLM provider key):\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=sk-xxxxxx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 2: Create a LangChain Prompt + Chain\n",
    "\n",
    "Here‚Äôs a basic LangChain chain using OpenAI's chat model.\n",
    "\n",
    "```python\n",
    "# langchain_chain.py\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def run_chain(user_input: str) -> str:\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    response = llm(messages)\n",
    "    return response.content\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 3: Create FastAPI App to Serve It\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from langchain_chain import run_chain\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/ask/\")\n",
    "def ask_llm(prompt: Prompt):\n",
    "    reply = run_chain(prompt.message)\n",
    "    return {\"response\": reply}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 4: Run FastAPI\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "‚û°Ô∏è Endpoint will be:\n",
    "`POST http://localhost:8000/ask/`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 5: Test It (Optional)\n",
    "\n",
    "You can test via Python, cURL, or Streamlit:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "res = requests.post(\"http://localhost:8000/ask/\", json={\"message\": \"Tell me a joke\"})\n",
    "print(res.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Component    | Purpose                          |\n",
    "| ------------ | -------------------------------- |\n",
    "| `langchain`  | Handles LLM logic                |\n",
    "| `FastAPI`    | Hosts LLM chains as API          |\n",
    "| `/ask/`      | API to send message to LangChain |\n",
    "| `ChatOpenAI` | Underlying LLM model             |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Optional Ideas\n",
    "\n",
    "* Swap OpenAI with Mistral, HuggingFace, Anthropic\n",
    "* Use LangChain agents or tools\n",
    "* Add streaming responses using `StreamingResponse`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
