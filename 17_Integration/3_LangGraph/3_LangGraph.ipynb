{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ðŸ“˜ Integration with LangGraph\n",
    "\n",
    "LangGraph is a **stateful, multi-agent framework** built on LangChain.\n",
    "You can wrap LangGraph workflows as FastAPI endpoints to run them via HTTP.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Step 1: Install Requirements\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn langgraph langchain openai\n",
    "```\n",
    "\n",
    "Set your OpenAI key (or use `.env`):\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=sk-xxxxxx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Step 2: Create a Simple LangGraph\n",
    "\n",
    "This example builds a **two-node graph**:\n",
    "\n",
    "* One node handles input\n",
    "* Second node returns an LLM response\n",
    "\n",
    "```python\n",
    "# graph_setup.py\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Define node\n",
    "def call_llm(state):\n",
    "    user_input = state[\"input\"]\n",
    "    response = llm([HumanMessage(content=user_input)])\n",
    "    return {\"output\": response.content}\n",
    "\n",
    "# Create graph\n",
    "def build_graph():\n",
    "    builder = StateGraph()\n",
    "    builder.add_node(\"llm_node\", call_llm)\n",
    "    builder.set_entry_point(\"llm_node\")\n",
    "    return builder.compile()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Step 3: Build FastAPI Server\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from graph_setup import build_graph\n",
    "\n",
    "app = FastAPI()\n",
    "graph = build_graph()\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/graph/\")\n",
    "async def run_graph(prompt: Prompt):\n",
    "    result = graph.invoke({\"input\": prompt.message})\n",
    "    return {\"response\": result[\"output\"]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Step 4: Run FastAPI\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Test endpoint:\n",
    "`POST http://localhost:8000/graph/`\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Step 5: Test via Python\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "res = requests.post(\"http://localhost:8000/graph/\", json={\"message\": \"What is LangGraph?\"})\n",
    "print(res.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary Table\n",
    "\n",
    "| Component    | Role                              |\n",
    "| ------------ | --------------------------------- |\n",
    "| `StateGraph` | LangGraph builder                 |\n",
    "| `call_llm`   | Node function using LangChain LLM |\n",
    "| `/graph/`    | FastAPI endpoint                  |\n",
    "| `ChatOpenAI` | Handles LLM logic                 |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Optional Enhancements\n",
    "\n",
    "* Add **multiple nodes** for decision logic\n",
    "* Use **tools or memory** inside LangGraph nodes\n",
    "* Stream output from FastAPI using `StreamingResponse`\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
